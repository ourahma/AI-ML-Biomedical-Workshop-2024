{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d49a82dc-7fad-417a-be2c-73733eb30776",
   "metadata": {},
   "source": [
    "# Object detection with Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa37bc-3bf8-4e4b-9c68-057f1399e796",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd0bdd8-e6bb-4c2d-80d7-997a656b8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import  transforms \n",
    "import torch\n",
    "from torch import no_grad\n",
    "import requests\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d939d1-1d3a-4755-8013-d909fc2151e9",
   "metadata": {},
   "source": [
    "## Auxiliary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94526de-0351-4674-bca1-c10e62983964",
   "metadata": {},
   "source": [
    "This function will assign a string name to a predicted class and eliminate predictions whose likelihood is under a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95c8d5fe-6308-4c90-b106-be1c828142e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(pred,threshold=0.8,objects=None ):\n",
    "    print(pred)\n",
    "    \"\"\"\n",
    "    This function will assign a string name to a predicted class and eliminate predictions whose likelihood  is under a threshold \n",
    "\n",
    "    Parameters:\n",
    "    - pred: A list containing a dictionary with predictions. The dictionary includes:\n",
    "        - 'labels': Tensor of predicted class indices.\n",
    "        - 'scores': Tensor of confidence scores for each prediction.\n",
    "        - 'boxes': Tensor of bounding box coordinates for each prediction.\n",
    "    - threshold: Confidence threshold to filter predictions. Only predictions with confidence above this threshold are kept.\n",
    "    - objects: Optional list of specific object classes to keep. If provided, only predictions of these classes are returned.\n",
    "    Returns:\n",
    "    - predicted_classes: A list of tuples, each containing:\n",
    "        - Class name (string)\n",
    "        - Confidence score (float)\n",
    "        - Bounding box coordinates (tuple of two points: (x1, y1), (x2, y2))\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the predictions into a more readable format:\n",
    "    # - COCO_INSTANCE_CATEGORY_NAMES[i]: Converts class index to a human-readable class name (e.g., 'person', 'car').\n",
    "    # - p: Confidence score of the prediction.\n",
    "    # - box: Bounding box coordinates of the detected object.\n",
    "    predicted_classes= [\n",
    "        (COCO_INSTANCE_CATEGORY_NAMES[i],p,[(box[0], box[1]), (box[2], box[3])]) \n",
    "        for i,p,box in zip(\n",
    "            list(pred[0]['labels'].numpy()),   # convert class indices to numpy list\n",
    "            pred[0]['scores'].detach().numpy(), # convert class scores to numpy list\n",
    "            list(pred[0]['boxes'].detach().numpy()))] # convert class boxes (x1,x2, y1,y2) to numpy list\n",
    "    predicted_classes=[  stuff  for stuff in predicted_classes  if stuff[1]>threshold ]\n",
    "    \n",
    "    if objects  and predicted_classes :\n",
    "        predicted_classes=[ (name, p, box) for name, p, box in predicted_classes if name in  objects ]\n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a4d83b-7495-4c34-a18c-41450cbb2e51",
   "metadata": {},
   "source": [
    "## Draw Box arround each Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f640ab-0ad8-49d6-b254-cd1886785f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_box(predicted_classes,image,rect_th= 10,text_size= 3,text_th=3):\n",
    "    \"\"\"\n",
    "    draws box around each object \n",
    "    \"\"\"\n",
    "    # Convert the image tensor to a NumPy array and adjust its format for OpenCV:\n",
    "    # 1. Transpose the image dimensions from (C, H, W) to (H, W, C).\n",
    "    # 2. Clip values to ensure they are within the range [0, 1].\n",
    "    # 3. Convert the image from RGB to BGR format (OpenCV uses BGR by default).\n",
    "    # 4. Scale the pixel values to the range [0, 255] and convert to uint8.\n",
    "    img=(np.clip(cv2.cvtColor(np.clip(image.numpy().transpose((1, 2, 0)),0,1), cv2.COLOR_RGB2BGR),0,1)*255).astype(np.uint8).copy()\n",
    "\n",
    "\n",
    "    # Iterate over each predicted object in the list\n",
    "    for predicted_class in predicted_classes:\n",
    "   \n",
    "        label=predicted_class[0]\n",
    "        probability=predicted_class[1]\n",
    "        box=predicted_class[2]\n",
    "\n",
    "        cv2.rectangle(img, box[0], box[1],(0, 255, 0), rect_th) # Draw Rectangle with the coordinates\n",
    "        cv2.putText(img,label, box[0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th) \n",
    "        cv2.putText(img,label+\": \"+str(round(probability,2)), box[0],  cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    del(img)\n",
    "    del(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b481f2f-ec40-471c-bbf0-5a35e974a743",
   "metadata": {},
   "source": [
    "#### Free up some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd51705-d0e8-441e-bacc-5d40d1f76dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_RAM(image_=False):\n",
    "    global image, img, pred\n",
    "    torch.cuda.empty_cache()\n",
    "    del(img)\n",
    "    del(pred)\n",
    "    if image_:\n",
    "        image.close()\n",
    "        del(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d389ed-420a-4a0a-8c71-cea9de5c0afa",
   "metadata": {},
   "source": [
    "## Load Pre-trained Faster R-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6c6c8-8689-4ee3-bbb5-64da53abce61",
   "metadata": {},
   "source": [
    "<a href='https://arxiv.org/abs/1506.01497'>Faster R-CNN</a> is a model that predicts both bounding boxes and class scores for potential objects in the image  pre-trained on <a href=\"https://cocodataset.org/\">COCO<a>. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e33135-c8a4-4c60-ba64-e31bd8a7a61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN)\n",
    "# The model is pre-trained on the COCO dataset, which includes 80 object classes.\n",
    "model_ = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "# Set the model to evaluation mode\n",
    "# This is important because some layers (e.g., dropout, batch normalization) behave differently\n",
    "# during training and evaluation. Setting the model to evaluation mode ensures consistent behavior.\n",
    "model_.eval()\n",
    "# Freeze all the parameters in the model to prevent them from being updated during training\n",
    "# This is useful when using the model for inference or fine-tuning only specific layers.\n",
    "for name, param in model_.named_parameters():\n",
    "    param.requires_grad = False # Disable gradient computation for this parameter\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1af5af18-c5d4-4e68-b540-e1c7c9caacc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    # Use torch.no_grad() to disable gradient computation\n",
    "    # This is important because:\n",
    "    # 1. It reduces memory usage by not storing intermediate values for backpropagation.\n",
    "    # 2. It speeds up computation since gradients are not calculated.\n",
    "    # 3. It ensures that the model's parameters are not updated during inference.\n",
    "    with torch.no_grad():\n",
    "        yhat= model_(x)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef9b89c-2b49-4d5a-aa33-e46d47acad47",
   "metadata": {},
   "source": [
    "## the classes used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a43c1f5-6756-420f-b755-416a67b4ec8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "len(COCO_INSTANCE_CATEGORY_NAMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4776a52d-79ad-47b7-88df-cedaaee49ccc",
   "metadata": {},
   "source": [
    "## Object localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9959c77b-bfc6-49cf-9051-75ccb62ab9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"test_images/desk.jpeg\"\n",
    "half = 0.5\n",
    "\n",
    "image = Image.open(img_path)\n",
    "\n",
    "image.resize(\n",
    "    [int(half * s) for s in image.size]\n",
    ")\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed7fb00-8676-40b4-81b2-481ef7252927",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
